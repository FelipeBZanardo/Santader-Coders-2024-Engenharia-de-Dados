{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e40efcf0-60e4-48cb-bb46-2b59056ae041",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Introdução Pyspark\n",
    "\n",
    "Pandas e PySpark são bibliotecas populares no ecossistema Python para trabalhar com dados estruturados. Eles oferecem recursos de manipulação e análise de dados, mas diferem significativamente em termos de escalabilidade, desempenho e recursos de computação distribuída. \n",
    "\n",
    "**1. Escalabilidade:**\n",
    "\n",
    "- **Pandas:** O Pandas opera na memória e é adequado para lidar com conjuntos de dados menores que cabem na RAM disponível. Ele é projetado principalmente para processamento em uma única máquina.\n",
    "   \n",
    "- **PySpark:** O PySpark, por outro lado, foi projetado para computação distribuída e pode lidar com grandes conjuntos de dados que excedem a capacidade de memória de uma única máquina. Ele distribui dados por um cluster de máquinas, permitindo o processamento paralelo.\n",
    "\n",
    "**2. Desempenho:**\n",
    "\n",
    "- **Pandas:** O Pandas é otimizado para processamento em uma única máquina, o que o torna muito eficiente para conjuntos de dados de pequeno e médio porte. No entanto, pode ter dificuldades com conjuntos de dados maiores devido a limitações de memória.\n",
    "   \n",
    "- **PySpark:** o desempenho do PySpark é dimensionado de acordo com o tamanho do cluster, tornando-o adequado para processamento de big data. Ele aproveita mecanismos de computação distribuída como Apache Spark para alcançar alto desempenho.\n",
    "\n",
    "**3. Fácil de usar:**\n",
    "\n",
    "- **Pandas:** O Pandas fornece uma API intuitiva e fácil de usar que é amplamente adotada por analistas de dados e cientistas. É conhecido por sua facilidade de uso e poderosos recursos de manipulação de dados.\n",
    "   \n",
    "- **PySpark:** a API do PySpark é semelhante à do Pandas, facilitando a transição dos usuários do Pandas. No entanto, configurar um cluster Spark e lidar com conceitos de computação distribuída pode ser mais complexo.\n",
    "\n",
    "**4. Criação de DataFrame:**\n",
    "\n",
    "- **Pandas:** Pandas DataFrames são criados a partir de várias fontes de dados, como arquivos CSV, planilhas Excel, bancos de dados SQL e dicionários Python.\n",
    "\n",
    "- **PySpark:** PySpark DataFrames normalmente são criados a partir de fontes de dados distribuídas, como HDFS, Apache Hive, Apache HBase ou RDDs (Resilient Distributed Datasets) existentes.\n",
    "\n",
    "**5. Casos de uso:**\n",
    "\n",
    "- **Pandas:** O Pandas é ideal para exploração, análise e pré-processamento de dados em conjuntos de dados menores, especialmente em um ambiente de máquina única. É uma ferramenta indispensável para muitos cientistas de dados.\n",
    "\n",
    "- **PySpark:** O PySpark foi projetado para processamento de big data e é mais adequado para situações em que os dados não cabem na memória ou quando você precisa aproveitar a computação distribuída para tarefas como ETL (Extrair, Transformar, Carregar) e tarefas de grande porte. análise de dados em escala.\n",
    "\n",
    "Em resumo, Pandas e PySpark DataFrames atendem a propósitos diferentes no ecossistema de processamento de dados. O Pandas é excelente para trabalhar com conjuntos de dados de pequeno e médio porte em uma única máquina, enquanto o PySpark é excelente para lidar com tarefas de processamento de dados distribuídos em grande escala. A escolha entre eles depende do tamanho específico dos dados e dos requisitos de processamento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dda6ed83-18c4-4417-b140-24bf80fa70d8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Exemplos Pyspark x Pandas\n",
    "\n",
    "Vamos nos aprofundar nas diferenças entre Pandas e PySpark DataFrames e mostrar várias operações usando ambas as bibliotecas, incluindo leitura de arquivos CSV e JSON, adição de colunas, filtragem de dados e remoção de duplicatas.\n",
    "\n",
    "**1. Lendo arquivos CSV:**\n",
    "\n",
    "\n",
    "- **Pandas:**\n",
    "\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "\n",
    "   pandas_df = pd.read_csv('data.csv')\n",
    "   ```\n",
    "\n",
    "- **PySpark:**\n",
    "\n",
    "   ```python\n",
    "   from pyspark.sql import SparkSession\n",
    "\n",
    "   spark = SparkSession.builder.appName('example').getOrCreate()\n",
    "   \n",
    "   pyspark_df = spark.read.csv('data.csv', header=True, inferSchema=True)\n",
    "   ```\n",
    "\n",
    "**2. Lendo JSON:**\n",
    "\n",
    "   - **Pandas:**\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "\n",
    "   pandas_df = pd.read_json('data.json')\n",
    "   ```\n",
    "\n",
    "   - **PySpark:**\n",
    "\n",
    "   ```python\n",
    "   from pyspark.sql import SparkSession\n",
    "   \n",
    "   spark = SparkSession.builder.appName('example').getOrCreate()\n",
    "\n",
    "   pyspark_df = spark.read.json('data.json')\n",
    "   ```\n",
    "\n",
    "**3. Adicionando colunas:**\n",
    "\n",
    "   - **Pandas:**\n",
    "\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "   \n",
    "   pandas_df['New_Column'] = [1, 2, 3]\n",
    "   ```\n",
    "\n",
    "   - **PySpark:**\n",
    "\n",
    "   ```python\n",
    "   from pyspark.sql.functions import lit\n",
    "\n",
    "   pyspark_df = pyspark_df.withColumn('New_Column', lit(1))\n",
    "   ```\n",
    "\n",
    "**4. Filtrando os dados:**\n",
    "\n",
    "   - **Pandas:**\n",
    "\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "\n",
    "   filtered_df = pandas_df[pandas_df['Age'] > 25]\n",
    "   ```\n",
    "\n",
    "   - **PySpark:**\n",
    "\n",
    "   ```python\n",
    "   from pyspark.sql.functions import col\n",
    "\n",
    "   filtered_df = pyspark_df.filter(col('Age') > 25)\n",
    "   ```\n",
    "\n",
    "**5. Excluindo duplicatas:**\n",
    "\n",
    "   - **Pandas:**\n",
    "\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "   pandas_df = pandas_df.drop_duplicates()\n",
    "   ```\n",
    "\n",
    "   - **PySpark:**\n",
    "\n",
    "   ```python\n",
    "   \n",
    "   pyspark_df = pyspark_df.dropDuplicates()\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9a16d9b-bccb-446c-97eb-3d7edd3fd084",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Persistência de Dados\n",
    "\n",
    "Persistir dados em vários formatos de arquivo como CSV, JSON, Parquet e outros é uma tarefa comum no processamento e análise de dados e no dia a dia de um Engenheiro de Dados. Python, Pandas e PySpark fornecem ferramentas e bibliotecas poderosas para fazer isso.\n",
    "\n",
    "**1. CSV (valores separados por vírgula):**\n",
    "\n",
    "**Exemplo - Escrevendo CSV Python:**\n",
    "\n",
    "```python\n",
    "import csv\n",
    "\n",
    "data = [\n",
    "    [\"Name\", \"Age\", \"City\"],\n",
    "    [\"Alice\", 25, \"New York\"],\n",
    "    [\"Bob\", 30, \"Los Angeles\"],\n",
    "    [\"Charlie\", 28, \"Chicago\"]\n",
    "]\n",
    "\n",
    "with open(\"data.csv\", mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(data)\n",
    "```\n",
    "**Exemplo - Escrevendo CSV com Pandas:**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"Name\": [\"Alice\", \"Bob\", \"Charlie\"],\n",
    "    \"Age\": [25, 30, 28],\n",
    "    \"City\": [\"New York\", \"Los Angeles\", \"Chicago\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"data.csv\", index=False)\n",
    "```\n",
    "\n",
    "\n",
    "**2. JSON :**\n",
    "\n",
    "\n",
    "**Exemplo - Escrevendo JSON Python:**\n",
    "\n",
    "```python\n",
    "import json\n",
    "\n",
    "data = {\n",
    "    \"Name\": \"Alice\",\n",
    "    \"Age\": 25,\n",
    "    \"City\": \"New York\"\n",
    "}\n",
    "\n",
    "with open(\"data.json\", 'w') as file:\n",
    "    json.dump(data, file)\n",
    "```\n",
    "**Exemplo - Escrevendo JSON com Pandas:**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"Name\": [\"Alice\", \"Bob\", \"Charlie\"],\n",
    "    \"Age\": [25, 30, 28],\n",
    "    \"City\": [\"New York\", \"Los Angeles\", \"Chicago\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_json(\"data.json\", orient=\"records\")\n",
    "```\n",
    "**3. Parquet:**\n",
    "\n",
    "Parquet é um formato de arquivo de armazenamento colunar altamente eficiente para processamento de big data. No PySpark, você pode usar o módulo `pyspark.sql` para ler e escrever arquivos Parquet.\n",
    "\n",
    "**Exemplo - Escrevendo Parquet com PySpark:**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ParquetExample\").getOrCreate()\n",
    "\n",
    "data = [(\"Alice\", 25, \"New York\"), (\"Bob\", 30, \"Los Angeles\"), (\"Charlie\", 28, \"Chicago\")]\n",
    "columns = [\"Name\", \"Age\", \"City\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.write.parquet(\"data.parquet\")\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "**4. Outros formatos de arquivo:**\n",
    "\n",
    "Existem muitos outros formatos de arquivo comumente usados para persistência de dados, como Avro, ORC e Feather. O PySpark também suporta muitos desses formatos e você pode usá-los de forma semelhante à forma como demonstramos o Parquet.\n",
    "\n",
    "**Exemplo - Escrevendo Avro com PySpark:**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"AvroExample\").getOrCreate()\n",
    "\n",
    "data = [(\"Alice\", 25, \"New York\"), (\"Bob\", 30, \"Los Angeles\"), (\"Charlie\", 28, \"Chicago\")]\n",
    "columns = [\"Name\", \"Age\", \"City\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.write.format(\"avro\").save(\"data.avro\")\n",
    "spark.stop()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79560c8c-fa97-4b1d-a8ec-e40167000578",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Tipos de Arquivos para Big Data\n",
    "\n",
    "### 1. Parquet\n",
    "\n",
    "**Características:**\n",
    "- Formato de arquivo colunar e eficiente para leitura/gravação.\n",
    "- Compressão eficiente, o que resulta em menor uso de armazenamento.\n",
    "- Projetado para ser compatível com sistemas de processamento de dados distribuídos, como Hadoop e Spark.\n",
    "- Suporta esquemas complexos e aninhados.\n",
    "- Ideal para consultas analíticas rápidas em grandes conjuntos de dados.\n",
    "\n",
    "### 2. Avro\n",
    "\n",
    "**Características:**\n",
    "- Formato de arquivo serializado e compactado.\n",
    "- Suporta esquemas complexos e evolutivos.\n",
    "- Ideal para troca de dados entre diferentes sistemas, devido à sua portabilidade e interoperabilidade.\n",
    "- Pode ser utilizado para streaming de dados devido à sua capacidade de serialização rápida e eficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6362ea71-d93f-4446-86f5-31c7a49b5a95",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ACID\n",
    "\n",
    "ACID é um acrônimo que representa um conjunto de propriedades que garantem a integridade dos dados em sistemas de banco de dados. Essas propriedades são essenciais para garantir que as transações sejam executadas de forma confiável, mesmo em ambientes distribuídos e concorrentes. Aqui está o significado de cada letra do acrônimo:\n",
    "\n",
    "### Atomicidade (Atomicity)\n",
    "\n",
    "A atomicidade garante que uma transação seja tratada como uma unidade indivisível de trabalho. Isso significa que todas as operações dentro da transação são concluídas com sucesso ou nenhuma delas é executada. Se uma parte da transação falhar, todas as operações anteriores devem ser revertidas, deixando o banco de dados em um estado consistente e pré-transação.\n",
    "\n",
    "### Consistência (Consistency)\n",
    "\n",
    "A consistência garante que apenas transações válidas, que preservem a integridade do banco de dados, sejam permitidas para execução. Isso significa que qualquer transação que leve o banco de dados de um estado consistente para outro estado consistente é aceita, enquanto as transações que violam as regras de integridade são rejeitadas.\n",
    "\n",
    "### Isolamento (Isolation)\n",
    "\n",
    "O isolamento garante que o resultado de uma transação seja independente de outras transações concorrentes. Isso significa que, mesmo em um ambiente com múltiplas transações ocorrendo simultaneamente, cada transação deve ser executada como se fosse a única transação no sistema. O isolamento previne que uma transação veja dados parcialmente atualizados ou temporários de outras transações.\n",
    "\n",
    "### Durabilidade (Durability)\n",
    "\n",
    "A durabilidade garante que os resultados de uma transação persistam no sistema mesmo após uma falha, como queda de energia ou falha de hardware. Isso significa que uma vez que uma transação é confirmada, as alterações feitas por essa transação são permanentemente gravadas no banco de dados e não serão perdidas, mesmo em caso de falha do sistema.\n",
    "\n",
    "Em conjunto, essas propriedades ACID garantem que as transações sejam executadas com segurança, consistência e confiabilidade em sistemas de banco de dados, proporcionando uma base sólida para o armazenamento e manipulação de dados críticos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6719895-d169-4869-96c6-43a6a7911bce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Novas soluções emergentes\n",
    "\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*GVIUOecJk2NmCdy3KIs52w.png\" style=\"border: 1px solid #aaa; border-radius: 10px 10px 10px 10px; box-shadow: 5px 5px 5px #aaa\"/>\n",
    "\n",
    "### Apache Hudi\n",
    "\n",
    "O Apache Hudi, que significa Hadoop Upserts Deletes & Incrementals, foi criado pela Uber Engineering e aberto para a comunidade em 2019. A motivação por trás da criação do Hudi foi resolver desafios específicos enfrentados pela Uber em seus pipelines de dados em grande escala. Eles precisavam de uma solução que permitisse ingestão incremental, atualização e exclusão eficientes de dados em ambientes distribuídos.\n",
    "\n",
    "O Hudi foi projetado para ser uma camada de abstração sobre sistemas de armazenamento distribuídos, como o Apache Hadoop e o Apache Spark, fornecendo suporte para operações de dados em tempo real e batch. Ele permite a criação de conjuntos de dados imutáveis e históricos, garantindo a consistência e a confiabilidade dos dados em pipelines de dados complexos.\n",
    "\n",
    "**Características:**\n",
    "- Framework open-source para gerenciamento de dados em grandes clusters Hadoop.\n",
    "- Suporta operações de ingestão incremental, atualização e exclusão de dados.\n",
    "- Permite a criação de conjuntos de dados imutáveis e históricos.\n",
    "- Integrado com ecossistema Hadoop, Spark e outras ferramentas de Big Data.\n",
    "- Facilita a construção de pipelines de dados robustos e confiáveis.\n",
    "\n",
    "\n",
    "### Apache Iceberg\n",
    "\n",
    "O Apache Iceberg foi criado pela Netflix em 2018 e doado para a Apache Software Foundation como um projeto de código aberto. A Netflix enfrentava desafios relacionados à evolução de esquemas e à eficiência na manipulação de grandes tabelas de dados em seus pipelines de processamento de dados.\n",
    "\n",
    "O Iceberg foi desenvolvido para fornecer uma estrutura de dados eficiente e escalável para tabelas grandes e mutáveis. Ele introduziu conceitos como transações ACID para operações de escrita, suporte para evolução de esquemas sem interromper consultas em andamento e metadados detalhados para otimizar consultas e gerenciar versões de tabelas.\n",
    "\n",
    "**Características:**\n",
    "- Estrutura de dados de código aberto para tabelas grandes e mutáveis.\n",
    "- Oferece transações ACID para operações de escrita.\n",
    "- Suporta a evolução de esquemas sem interromper consultas em andamento.\n",
    "- Fornece metadados detalhados para otimizar consultas e gerenciar versões de tabelas.\n",
    "- Ideal para cenários em que é necessário garantir a consistência e a integridade dos dados em tabelas grandes e compartilhadas.\n",
    "\n",
    "### Delta Lake\n",
    "\n",
    "O Delta Lake foi criado pela Databricks e lançado em 2019 como um projeto de código aberto. A Databricks identificou a necessidade de adicionar transações ACID e histórico de versões a data lakes existentes para suportar casos de uso mais avançados, como machine learning e análise em tempo real.\n",
    "\n",
    "O Delta Lake é construído sobre o Apache Spark e oferece recursos como transações ACID para operações de escrita, ingestão de dados em tempo real e batch, e compatibilidade com ferramentas de análise de dados existentes. Ele foi projetado para ser uma camada de armazenamento confiável e escalável para data lakes, facilitando a construção de pipelines de dados robustos e confiáveis.\n",
    "\n",
    "**Características:**\n",
    "- Solução de armazenamento de dados de código aberto construída sobre o Apache Spark.\n",
    "- Adiciona transações ACID e histórico de versões a data lakes existentes.\n",
    "- Suporta ingestão de dados em tempo real e batch.\n",
    "- Compatível com ferramentas de análise de dados existentes, como Spark, Presto e Hive.\n",
    "- Facilita a construção de pipelines de dados confiáveis e tolerantes a falhas.\n",
    "\n",
    "Essas três tecnologias, Apache Hudi, Apache Iceberg e Delta Lake, surgiram como resposta a desafios específicos enfrentados por empresas que lidam com grandes volumes de dados em ambientes distribuídos. Cada uma delas introduziu inovações importantes no espaço de Big Data, fornecendo soluções para problemas complexos relacionados à integridade, eficiência e escalabilidade dos dados.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://celerdata.com/hs-fs/hubfs/delta%20lake_iceberg_hudi_apache_data%20lake.png?width=2427&height=1246&name=delta%20lake_iceberg_hudi_apache_data%20lake.png\" style=\"border: 1px solid #aaa; border-radius: 10px 10px 10px 10px; box-shadow: 5px 5px 5px #aaa\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b29a4bf-cec2-48be-9416-990c09ac7d2f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<img src=\"https://www.dremio.com/wp-content/uploads/2022/06/Iceberg@2x-768x582.png\" style=\"border: 1px solid #aaa; border-radius: 10px 10px 10px 10px; box-shadow: 5px 5px 5px #aaa\"/>\n",
    "<img src=\"https://www.dremio.com/wp-content/uploads/2022/06/Apache-Hudi@2x-768x582.png\" style=\"border: 1px solid #aaa; border-radius: 10px 10px 10px 10px; box-shadow: 5px 5px 5px #aaa\"/>\n",
    "<img src=\"https://www.dremio.com/wp-content/uploads/2022/06/Delta-Lake@2x-768x582.png\" style=\"border: 1px solid #aaa; border-radius: 10px 10px 10px 10px; box-shadow: 5px 5px 5px #aaa\"/>\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2488438975114110,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "AULA_4_DE",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
