{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e13a09e7-c89d-47ee-8ebc-11c0af4ffdd7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1. Instalação de Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80ba8ee1-204e-4f7a-a4bd-fc1d2e2e3a40",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting kafka-python\n  Using cached kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\nInstalling collected packages: kafka-python\nSuccessfully installed kafka-python-2.0.2\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "pip install kafka-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bee04f4b-c4d8-4c07-a0ac-dc0c165fc4f7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5949ff35-6d75-4a88-ae79-537ea49f229a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "from pyspark.sql.utils import AnalysisException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a2a226e-18a6-472f-8db4-be7594092e50",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 3. Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d76e2604-a483-4bea-93d0-51d7a29e0501",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "topico = 'meu-topico'\n",
    "\n",
    "path = '/Filestore/projeto/dados_raw'\n",
    "\n",
    "schema = StructType([\n",
    "        StructField(\"source\", StructType((\n",
    "            StructField(\"id\", StringType(), True),\n",
    "            StructField(\"name\", StringType(), True)\n",
    "        )), True),\n",
    "        StructField(\"author\", StringType(), True),\n",
    "        StructField(\"title\", StringType(), True),\n",
    "        StructField(\"description\", StringType(), True),\n",
    "        StructField(\"url\", StringType(), True),\n",
    "        StructField(\"urlToImage\", StringType(), True),\n",
    "        StructField(\"publishedAt\", StringType(), True),\n",
    "        StructField(\"content\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cf09722-0ead-4f50-b59d-8d16440294e9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 4. Persistência dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d599996c-2edd-46d1-aa09-47a9f3fadf43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def obter_dados_nao_salvos(df):\n",
    "  try:\n",
    "    df_leitura = spark.read.parquet(path)\n",
    "    df_left_join = df.alias(\"df1\").join(df_leitura.alias(\"df2\"), on=\"url\", how=\"left\")\n",
    "    left_exclude = df_left_join.filter(col(\"df2.url\").isNull()).select(\"df1.*\")\n",
    "    return left_exclude\n",
    "  except AnalysisException:\n",
    "    return df\n",
    "\n",
    "def processamento_raw(df):\n",
    "  df = df.dropDuplicates(['url'])\n",
    "  df = df.withColumn('data_processamento', current_timestamp())\n",
    "  return df\n",
    "\n",
    "def save_data(df):\n",
    "  df.write.format('delta').mode('append').parquet(path)\n",
    "  print(f\"{df.count()} novas notícias foram persistidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a44685eb-620e-4b07-a5d9-0d0e71b0939c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 5. Recebe os dados e persiste no arquivo parquet a cada 50 mensagens lidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0e01ff6-573d-4c2f-a5c1-8e3f18b742c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 novas notícias foram persistidas\n47 novas notícias foram persistidas\n50 novas notícias foram persistidas\n188 novas notícias foram persistidas\n0 novas notícias foram persistidas\n0 novas notícias foram persistidas\n0 novas notícias foram persistidas\n0 novas notícias foram persistidas\n0 novas notícias foram persistidas\n0 novas notícias foram persistidas\n0 novas notícias foram persistidas\n"
     ]
    }
   ],
   "source": [
    "# Função de desserialização\n",
    "def deserializer(message):\n",
    "    return json.loads(message.decode('utf-8'))\n",
    "\n",
    "# Criando o consumidor no grupo \"grupo-de-consumo-1\"\n",
    "consumer = KafkaConsumer(\n",
    "    topico,\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    auto_offset_reset='latest',\n",
    "    # group_id='grupo-de-consumo-1',  # Definindo o grupo de consumidores\n",
    "    value_deserializer=deserializer\n",
    ")\n",
    "\n",
    "mensagens = []\n",
    "contador = 0\n",
    "for message in consumer:\n",
    "    contador += 1\n",
    "    valor = message.value\n",
    "    mensagens.append(valor)\n",
    "    \n",
    "    if contador == 50:\n",
    "        contador = 0\n",
    "        df = spark.createDataFrame(mensagens, schema)\n",
    "        df_nova_noticia = obter_dados_nao_salvos(processamento_raw(df))\n",
    "        save_data(df_nova_noticia)\n",
    "\n",
    "# Fechando o consumer\n",
    "consumer.close()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Consumidor - Kafka",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
