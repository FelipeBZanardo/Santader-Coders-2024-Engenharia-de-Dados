{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7091543b-4941-4623-a4b7-c41a16e7212d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/databricks/jars/----ws_3_3--mvn--hadoop3--org.apache.logging.log4j--log4j-slf4j-impl--org.apache.logging.log4j__log4j-slf4j-impl__2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/databricks/driver/kafka_2.12-3.6.2/libs/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[2024-10-20 16:31:07,432] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)\n[2024-10-20 16:31:08,952] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)\n[2024-10-20 16:31:09,313] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)\n[2024-10-20 16:31:09,320] INFO starting (kafka.server.KafkaServer)\n[2024-10-20 16:31:09,321] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)\n[2024-10-20 16:31:09,380] INFO [ZooKeeperClient Kafka server] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)\n[2024-10-20 16:31:09,401] INFO Client environment:zookeeper.version=3.6.2--803c7f1a12f85978cb049af5e4ef23bd8b688715, built on 09/04/2020 12:44 GMT (org.apache.zookeeper.ZooKeeper)\n[2024-10-20 16:31:09,401] INFO Client environment:host.name=1020-152736-6tbvonfb-10-172-219-243 (org.apache.zookeeper.ZooKeeper)\n[2024-10-20 16:31:09,402] INFO Client environment:java.version=1.8.0_382 (org.apache.zookeeper.ZooKeeper)\n[2024-10-20 16:31:09,402] INFO Client environment:java.vendor=Azul Systems, Inc. (org.apache.zookeeper.ZooKeeper)\n[2024-10-20 16:31:09,402] INFO Client environment:java.home=/usr/lib/jvm/zulu8-ca-amd64/jre (org.apache.zookeeper.ZooKeeper)\n[2024-10-20 16:31:09,402] INFO Client environment:java.class.path=/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/log4j/driver:/databricks/hive/conf:/databricks/spark/dbconf/hadoop:/databricks/jars/----ws_3_3--mllib--libmllib_resources.jar:/databricks/jars/----ws_3_3--third_party--mssql--mssql-hive-2.3__hadoop-3.2_2.12---2072066003--io.netty__netty-tcnative-classes__2.0.48.Final.jar:/databricks/jars/----ws_3_3--sql--hive-thriftserver--hive-thriftserver-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/common--advanced-feature--advanced-feature-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--armeria--armeria_shaded---924439155--net.bytebuddy__byte-buddy__1.12.6.jar:/databricks/jars/common--encryption--cpk-encryption-conf-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.glassfish.hk2.external--aopalliance-repackaged--org.glassfish.hk2.external__aopalliance-repackaged__2.6.1.jar:/databricks/jars/third_party--gcp-java--gcp-java_shaded--992224560--com.google.protobuf__protobuf-java__3.15.5.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--net.sourceforge.f2j--arpack_combined_all--net.sourceforge.f2j__arpack_combined_all__0.1.jar:/databricks/jars/common--rpc--ssl--ssl-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--gcs-private--gcs-shaded_common--1860799066--com.google.flogger__flogger-system-backend__0.5.1.jar:/databricks/jars/common--conf--cluster-spark--cluster-spark-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--armeria--armeria_shaded--1805148092--io.netty__netty-resolver__4.1.77.Final.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--jakarta.ws.rs--jakarta.ws.rs-api--jakarta.ws.rs__jakarta.ws.rs-api__2.1.6.jar:/databricks/jars/third_party--gcp-java--gcp-java_shaded--329952121--com.google.cloud__google-cloud-storage__1.113.14-sp.4.jar:/databricks/jars/third_party--armeria--armeria_shaded--2018211991--com.fasterxml.jackson.core__jackson-core__2.13.5.jar:/databricks/jars/common--jetty--client--client-spark_3.3_2.12_deploy.jar:/databricks/jars/dbfs--exceptions--exceptions-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--javax.transaction--transaction-api--javax.transaction__transaction-api__1.1.jar:/databricks/jars/third_party--gcp-java--gcp-java_shaded---32986530--com.google.code.gson__gson__2.8.9.jar:/databricks/jars/common--context--attr_key-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.glassfish.jersey.core--jersey-common--org.glassfish.jersey.core__jersey-common__2.36.jar:/databricks/jars/common--uc-volumes--volume_path-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--gcp-java--gcp-java_shaded---1258092488--com.google.apis__google-api-services-cloudresourcemanager__v1-rev20201111-1.30.10.jar:/databricks/jars/common--database--datasource--standalone-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.scalacheck--scalacheck_2.12--org.scalacheck__scalacheck_2.12__1.14.2.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--com.twitter--util-app_2.12--com.twitter__util-app_2.12__7.1.0.jar:/databricks/jars/----ws_3_3--third_party--kinesis-sdk--kinesis-sdk---553335969--software.amazon.awssdk__metrics-spi__2.17.190.jar:/databricks/jars/common--libraries--libraries-spark_3.3_2.12_deploy.jar:/databricks/jars/common--logging--filters--service--service-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.apache.derby--derby--org.apache.derby__derby__10.14.2.0.jar:/databricks/jars/third_party--opencensus-shaded--opencensus_shaded--1770584804--com.lmax__disruptor__3.4.2.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-shaded_20180625_3682417-spark_3.3_2.12--719397394--com.microsoft.azure__azure-keyvault-core__1.0.0.jar:/databricks/jars/----jackson_annotations_shaded--libjackson-annotations.jar:/databricks/jars/third_party--grpc-netty--grpc-netty_shaded---1010389895--io.netty__netty-tcnative-boringssl-static-osx-aarch_64__2.0.52.Final.jar:/databricks/jars/common--compression--compression-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gcs-dependencies-shaded---13112641--org.codehaus.mojo__animal-sniffer-annotations__1.19.jar:/databricks/jars/common--rpc--conf--conf-spark_3.3_2.12_deploy.jar:/databricks/jars/common--encryption--cpk-deps-shaded---1375035343--org.reactivestreams__reactive-streams__1.0.3.jar:/databricks/jars/third_party--gcp-java--gcp-java_shaded--705909857--com.google.http-client__google-http-client-jackson2__1.39.0.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gcs-dependencies-shaded--945576458--io.perfmark__perfmark-api__0.23.0.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.1.jar:/databricks/jars/third_party--gcp-java--gcp-java_shaded--1826029026--io.grpc__grpc-grpclb__1.34.1.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.glassfish.hk2--hk2-utils--org.glassfish.hk2__hk2-utils__2.6.1.jar:/databricks/jars/third_party--gcp-java--gcp-java_shaded---2128655557--org.apache.commons__commons-lang3__3.5.jar:/databricks/jars/common--util--math-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--grpc-netty--grpc-netty_shaded---330209066--io.netty__netty-codec__4.1.77.Final.jar:/databricks/jars/common--logging--utils-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--net.sf.jpam--jpam--net.sf.jpam__jpam__1.1.jar:/databricks/jars/----ws_3_3--sql--core--proto_2.12_deploy.jar:/databricks/jars/common--util--item-state-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--grpc-netty--grpc-netty_shaded--247627786--io.netty__netty-handler__4.1.77.Final.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--com.amazonaws--aws-java-sdk-directory--com.amazonaws__aws-java-sdk-directory__1.12.189.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--commons-io--commons-io--commons-io__commons-io__2.11.0.jar:/databricks/jars/common--instrumentation--servlets--servlets-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--grpc-netty--grpc-netty_shaded--103274271--io.netty__netty-codec-http__4.1.77.Final.jar:/databricks/jars/----ws_3_3--third_party--mssql--mssql-hive-2.3__hadoop-3.2_2.12---1288299919--com.microsoft.azure__msal4j-persistence-extension__1.1.0.jar:/databricks/jars/third_party--gcs-private--gcs-shaded_common---860840161--commons-codec__commons-codec__1.11.jar:/databricks/jars/common--rpc--metrics--opencensus-spark_3.3_2.12_deploy.jar:/databricks/jars/s3commit--api--api-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--jakarta.validation--jakarta.validation-api--jakarta.validation__jakarta.validation-api__2.0.2.jar:/databricks/jars/spark--warmuptracing--traced_proxies_3.3.jar:/databricks/jars/common--logging--filters--filters-spark_3.3_2.12_deploy.jar:/databricks/jars/common--cloudstorage--presigned-url-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--gcp-java--gcp-java_shaded---212539616--com.google.auth__google-auth-library-oauth2-http__1.15.0.jar:/databricks/jars/----glue-catalog-spark3.3-client--glue-catalog-client-common_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.codehaus.janino--commons-compiler--org.codehaus.janino__commons-compiler__3.0.16.jar:/databricks/jars/common--threading--threading-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gcs-connector-all-shaded-spark_3.3_2.12--1009477437--util-hadoop-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--gcs-private--gcs-shaded_common---1359320259--io.grpc__grpc-context__1.32.1.jar:/databricks/jars/third_party--gcp-java--gcp-java_shaded---1312913098--com.google.j2objc__j2objc-annotations__1.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-shaded_20180625_3682417-spark_3.3_2.12---771781154--javax.activation__activation__1.1.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.apache.parquet--parquet-encoding--org.apache.parquet__parquet-encoding__1.12.3-databricks-0009.jar:/databricks/jars/----ws_3_3--third_party--kinesis-sdk--kinesis-sdk--1735805516--software.amazon.awssdk__http-client-spi__2.17.190.jar:/databricks/jars/spark--driver--events-spark_3.3_2.12_deploy.jar:/databricks/jars/secret-manager--libsecret-manager-crypto.jar:/databricks/jars/spark--driver--safespark-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--third_party--mssql--mssql-hive-2.3__hadoop-3.2_2.12--787278699--io.netty__netty-codec-dns__4.1.77.Final.jar:/databricks/jars/common--cluster--instance-pool--instance-pool-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--gcs-private--gcs-shaded_common--450216080--com.google.android__annotations__4.1.1.4.jar:/databricks/jars/common--instrumentation--stack-trace-filter--stack-trace-filter-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--dev.ludovic.netlib--arpack--dev.ludovic.netlib__arpack__2.2.1.jar:/databricks/jars/common--util--emailclient-spark_3.3_2.12_deploy.jar:/databricks/jars/common--logging--tags--chauffeur--chauffeur-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--grpc-netty--grpc-netty_shaded---1570573510--io.netty__netty-tcnative-boringssl-static-linux-x86_64__2.0.52.Final.jar:/databricks/jars/common--logging--log4j--log4j-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--vendor--file-notification-common--libfile-notification-common_resources.jar:/databricks/jars/common--logging--tags--sqlgateway--sqlgateway-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--third_party--mssql--mssql-hive-2.3__hadoop-3.2_2.12--819614115--io.netty__netty-transport-native-kqueue-osx-x86_64__4.1.74.Final.jar:/databricks/jars/common--encryption--cpk-deps-shaded---1474950387--io.netty__netty-buffer__4.1.87.Final.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gcs-dependencies-shaded--485924234--io.grpc__grpc-api__1.37.0.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--com.amazonaws--aws-java-sdk-cognitoidentity--com.amazonaws__aws-java-sdk-cognitoidentity__1.12.189.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.eclipse.jetty--jetty-xml--org.eclipse.jetty__jetty-xml__9.4.46.v20220331.jar:/databricks/jars/----ws_3_3--graphx--graphx-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/common--database--migration--orchestration--orchestration-spark_3.3_2.12_deploy.jar:/databricks/jars/common--threading--future--future-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--gcp-java--gcp-java_shaded---84348661--com.google.api.grpc__proto-google-common-protos__2.1.0.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.typelevel--algebra_2.12--org.typelevel__algebra_2.12__2.0.1.jar:/databricks/jars/third_party--armeria--armeria_shaded--839560296--io.netty__netty-codec-haproxy__4.1.77.Final.jar:/databricks/jars/dbfs--utils--dbfs-utils-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.json4s--json4s-jackson_2.12--org.json4s__json4s-jackson_2.12__3.7.0-M11.jar:/databricks/jars/----ws_3_3--vendor--avro--kafka-clients_only_shaded-for-avro-hive-2.3__hadoop-3.2---614483296--org.apache.kafka__kafka-clients__7.4.0-ccs.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.apache.hive--hive-shims--org.apache.hive__hive-shims__2.3.9.jar:/databricks/jars/common--encryption--cpk-deps-shaded--1200635519--com.azure__azure-security-keyvault-keys__4.3.2.jar:/databricks/jars/common--conf--project-conf--project-conf-spark_3.3_2.12_deploy.jar:/databricks/jars/common--instrumentation--proc-meminfo-exporter--proc-meminfo-exporter-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.scala-sbt--test-interface--org.scala-sbt__test-interface__1.0.jar:/databricks/jars/common--dbsql-config--dbsql-config-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-shaded_20180625_3682417-spark_3.3_2.12--1908666790--com.squareup.retrofit2__converter-jackson__2.1.0.jar:/databricks/jars/feature-flag--client--common-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--io.delta--delta-sharing-spark_2.12--io.delta__delta-sharing-spark_2.12__0.6.11.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-shaded_20180625_3682417-spark_3.3_2.12---2053201302--aopalliance__aopalliance__1.0.jar:/databricks/jars/proto--logs--test-usage-log--test_usage_log_proto-spark_3.3_2.12-scalabp.jar:/databricks/jars/common--conf--project--project-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--com.amazonaws--aws-java-sdk-kinesis--com.amazonaws__aws-java-sdk-kinesis__1.12.189.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.postgresql--postgresql--org.postgresql__postgresql__42.3.3.jar:/databricks/jars/----ws_3_3--vendor--snowflake--libsnowflake_resources.jar:/databricks/jars/----ws_3_3--vendor--connect--server--spark_connect_proto_lib_shaded--2124703135--common-files-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/common--doughnut-buffer--doughnut-buffer-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--third_party--kinesis-sdk--kinesis-sdk---2077074427--com.typesafe.netty__netty-reactive-streams__2.0.5.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--net.snowflake--snowflake-jdbc--net.snowflake__snowflake-jdbc__3.13.33.jar:/databricks/jars/common--types--types-spark_3.3_2.12_deploy.jar:/databricks/jars/common--logging--redactor--usage_log_redactor-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--com.amazonaws--aws-java-sdk-opsworks--com.amazonaws__aws-java-sdk-opsworks__1.12.189.jar:/databricks/jars/common--jobs--run_id-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--gcs-private--gcs-shaded_common--1307550018--org.apache.httpcomponents__httpclient__4.5.13.jar:/databricks/jars/macros--sourcecode--sourcecode-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--third_party--kinesis-sdk--kinesis-sdk--671268491--software.amazon.awssdk__auth__2.17.190.jar:/databricks/jars/----ws_3_3--third_party--kinesis-sdk--kinesis-sdk--2124613692--software.amazon.awssdk__regions__2.17.190.jar:/databricks/jars/third_party--opencensus-shaded--opencensus_shaded---388675048--org.apache.httpcomponents__httpcore__4.4.1.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.typelevel--macro-compat_2.12--org.typelevel__macro-compat_2.12__1.1.1.jar:/databricks/jars/third_party--gcs-private--gcs-shaded_common---1536677093--com.google.protobuf__protobuf-java-util__3.14.0.jar:/databricks/jars/common--util--network-spark_3.3_2.12_deploy.jar:/databricks/jars/proto--logs--options_proto-spark_3.3_2.12-scalabp.jar:/databricks/jars/third_party--grpc-netty--grpc-netty_shaded--887644477--com.google.guava__listenablefuture__9999.0-empty-to-avoid-conflict-with-guava.jar:/databricks/jars/common--util--execution-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--com.amazonaws--aws-java-sdk-elasticache--com.amazonaws__aws-java-sdk-elasticache__1.12.189.jar:/databricks/jars/----ws_3_3--third_party--msal4j--msal4j-shaded--1445930268--net.minidev__json-smart__2.5.0.jar:/databricks/jars/----ws_3_3--third_party--mssql--mssql-hive-2.3__hadoop-3.2_2.12---407180191--io.projectreactor.netty__reactor-netty-http__1.0.20.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.apache.hive--hive-serde--org.apache.hive__hive-serde__2.3.9.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--io.dropwizard.metrics--metrics-jvm--io.dropwizard.metrics__metrics-jvm__4.1.1.jar:/databricks/jars/sqlgateway--history--api--api-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--gcp-java--gcp-java_shaded--1493944588--io.grpc__grpc-api__1.34.1.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.apache.hive--hive-cli--org.apache.hive__hive-cli__2.3.9.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--io.prometheus--simpleclient--io.prometheus__simpleclient__0.7.0.jar:/databricks/jars/third_party--gcp-java--gcp-java_shaded--1957504218--com.google.apis__google-api-services-deploymentmanager__v2-rev20201109-1.30.10.jar:/databricks/jars/workflow--workflow-spark_3.3_2.12_deploy.jar:/databricks/jars/proto--logs--activity--webapp--webapp_service_proto-spark_3.3_2.12-scalabp.jar:/databricks/jars/common--logging--audit-event--audit-event-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gcs-dependencies-shaded--22063857--com.google.errorprone__error_prone_annotations__2.3.4.jar:/databricks/jars/common--storage--storage-spark_3.3_2.12_deploy.jar:/databricks/jars/common--jobs--job_cluster_key-spark_3.3_2.12_deploy.jar:/databricks/jars/spark--pipelines--execution--core--core-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--gcs-private--gcs-shaded_common--1125868025--com.google.guava__guava__30.1-jre.jar:/databricks/jars/third_party--armeria--armeria_shaded---1002230996--com.fasterxml.jackson.core__jackson-annotations__2.13.5.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--com.amazonaws--amazon-kinesis-client--com.amazonaws__amazon-kinesis-client__1.12.0.jar:/databricks/jars/common--conf--core--core-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-shaded_20180625_3682417-spark_3.3_2.12--1428008859--com.fasterxml.jackson.core__jackson-annotations__2.7.0.jar:/databricks/jars/----ws_3_3--vendor--sql-azure-connectors--libsql-azure-connectors_resources.jar:/databricks/jars/third_party--grpc-netty--grpc-netty_shaded---744856947--com.google.code.findbugs__jsr305__3.0.2.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--com.google.flatbuffers--flatbuffers-java--com.google.flatbuffers__flatbuffers-java__1.12.0.jar:/databricks/jars/common--rpc--clienttype--clienttype-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--gcp-java--gcp-java_shaded--1606459201--org.codehaus.mojo__animal-sniffer-annotations__1.19.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gcs-dependencies-shaded---1373112910--io.grpc__grpc-netty-shaded__1.37.0.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--io.dropwizard.metrics--metrics-core--io.dropwizard.metrics__metrics-core__4.1.1.jar:/databricks/jars/----ws_3_3--third_party--kinesis-sdk--kinesis-sdk---722831861--software.amazon.awssdk__dynamodb__2.15.31.jar:/databricks/jars/common--chauffeur--execution--execution-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--com.amazonaws--jmespath-java--com.amazonaws__jmespath-java__1.12.189.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.glassfish.jersey.core--jersey-client--org.glassfish.jersey.core__jersey-client__2.36.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--io.netty--netty-tcnative-classes--io.netty__netty-tcnative-classes__2.0.48.Final.jar:/databricks/jars/----ws_3_3--safespark--udf--common--client-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--com.amazonaws--aws-java-sdk-datapipeline--com.amazonaws__aws-java-sdk-datapipeline__1.12.189.jar:/databricks/jars/proto--logs--pipelines--pipelines_proto-spark_3.3_2.12-scalabp.jar:/databricks/jars/common--logging--structured--proto-logger-configuration-spark_3.3_2.12_deploy.jar:/databricks/jars/proto--logs--service-request--http_headers_proto-spark_3.3_2.12-scalabp.jar:/databricks/jars/feature-flag--client--client-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--gcs-private--gcs-shaded_common---470888215--com.google.flogger__flogger__0.5.1.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gcs-connector-all-shaded-spark_3.3_2.12--1057051737--libgcsio_proto_library-speed.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.apache.curator--curator-framework--org.apache.curator__curator-framework__2.13.0.jar:/databricks/jars/----ws_3_3--sql--catalyst--libspark-sql-parser-compiled.jar:/databricks/jars/common--conf--parser--parser-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--grpc-netty--grpc-netty_shaded---18019359--io.netty__netty-transport-udt__4.1.77.Final.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gcs-dependencies-shaded---70021406--org.conscrypt__conscrypt-openjdk-uber__2.5.1.jar:/databricks/jars/----ws_3_3--third_party--kinesis-sdk--kinesis-sdk--19901040--software.amazon.awssdk__aws-query-protocol__2.17.190.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--com.amazonaws--aws-java-sdk-cognitosync--com.amazonaws__aws-java-sdk-cognitosync__1.12.189.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.eclipse.jetty.websocket--websocket-servlet--org.eclipse.jetty.websocket__websocket-servlet__9.4.46.v20220331.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.glassfish.jersey.containers--jersey-container-servlet--org.glassfish.jersey.containers__jersey-container-servlet__2.36.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.apache.ant--ant-launcher--org.apache.ant__ant-launcher__1.9.2.jar:/databricks/jars/common--annotations--threadsafety--threadsafety-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gcs-shaded-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.apache.parquet--parquet-hadoop--org.apache.parquet__parquet-hadoop__1.12.3-databricks-0009.jar:/databricks/jars/common--logging--tags--data-monitoring--data-monitoring-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--gcs-private--gcs-shaded_common---1724254641--com.google.http-client__google-http-client__1.39.0.jar:/databricks/jars/spark--safer--config_category_lib-spark_3.3_2.12_deploy.jar:/databricks/jars/common--rpc--rate-limiter--rate-limiter-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-shaded_20180625_3682417-spark_3.3_2.12---602169413--org.apache.httpcomponents__httpcore__4.4.11.jar:/databricks/jars/common--util--hostname-resolver-spark_3.3_2.12_deploy.jar:/databricks/jars/common--encryption--azure_key_vault_validator-spark_3.3_2.12_deploy.jar:/databricks/jars/common--build-info--build-info-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--third_party--kinesis-sdk--kinesis-sdk---1091084412--org.apache.kafka__connect-api__2.5.0.jar:/databricks/jars/----ws_3_3--third_party--kinesis-sdk--kinesis-sdk--1570108344--io.reactivex.rxjava2__rxjava__2.1.14.jar:/databricks/jars/third_party--gcp-java--gcp-java_shaded---55147804--com.google.api__gax-httpjson__0.79.0.jar:/databricks/jars/chauffeur-api--chauffeur-api-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--third_party--kinesis-sdk--kinesis-sdk---2059345175--software.amazon.awssdk__third-party-jackson-core__2.17.190.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--com.tdunning--json--com.tdunning__json__1.8.jar:/databricks/jars/----ws_3_3--patched-hive-with-glue--hive-exec_filtered---593920692--org.apache.hive__hive-exec-core__2.3.9.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--com.amazonaws--aws-java-sdk-ssm--com.amazonaws__aws-java-sdk-ssm__1.12.189.jar:/databricks/jars/common--jobs--trigger_id-spark_3.3_2.12_deploy.jar:/databricks/jars/proto--logs--billableusage--types_proto-spark_3.3_2.12-scalabp.jar:/databricks/jars/third_party--grpc-netty--grpc-netty_shaded---1083310950--io.net\n\n*** WARNING: max output size exceeded, skipping output. ***\n\nabricks/driver/kafka_2.12-3.6.2/bin/../libs/metrics-core-4.1.12.1.jar:/databricks/driver/kafka_2.12-3.6.2/bin/../libs/netty-buffer-4.1.100.Final.jar:/databricks/driver/kafka_2.12-3.6.2/bin/../libs/netty-codec-4.1.100.Final.jar:/databricks/driver/kafka_2.12-3.6.2/bin/../libs/netty-common-4.1.100.Final.jar:/databricks/driver/kafka_2.12-3.6.2/bin/../libs/netty-handler-4.1.100.Final.jar:/databricks/driver/kafka_2.12-3.6.2/bin/../libs/netty-resolver-4.1.100.Final.jar:/databricks/driver/kafka_2.12-3.6.2/bin/../libs/netty-transport-4.1.100.Final.jar:/databricks/driver/kafka_2.12-3.6.2/bin/../libs/netty-transport-classes-epoll-4.1.100.Final.jar:/databricks/driver/kafka_2.12-3.6.2/bin/../libs/netty-transport-native-epoll-4.1.100.Final.jar:/databricks/driver/kafka_2.12-3.6.2/bin/../libs/netty-transport-native-unix-common-4.1.100.Final.jar:/databricks/driver/kafka_2.12-3.6.2/bin/../libs/osgi-resource-locator-1.0.3.jar:/databricks/driver/kafka_2.12-3.6.2/bin/../libs/paranamer-2.8.jar:/databricks/driver/kafka_2.12-3.6.2/bin/../libs/pcollections-4.0.1.jar:/databricks/driver/kafka_2.12-3.6.2/bin/../libs/plexus-utils-3.3.1.jar:/databricks/driver/kafka_2.12-3.6.2/bin/../libs/reflections-0.10.2.jar:/databricks/driver/kafka_2.12-3.6.2/bin/../libs/reload4j-1.2.25.jar:/databricks/driver/kafka_2.12-3.6.2/bin/../libs/rocksdbjni-7.9.2.jar:/databricks/driver/kafka_2.12-3.6.2/bin/../libs/scala-collection-compat_2.12-2.10.0.jar:/databricks/driver/kafka_2.12-3.6.2/bin/../libs/scala-java8-compat_2.12-1.0.2.jar:/databricks/driver/kafka_2.12-3.6.2/bin/../libs/scala-library-2.12.18.jar:/databricks/driver/kafka_2.12-3.6.2/bin/../libs/scala-logging_2.12-3.9.4.jar:/databricks/driver/kafka_2.12-3.6.2/bin/../libs/scala-reflect-2.12.18.jar:/databricks/driver/kafka_2.12-3.6.2/bin/../libs/slf4j-api-1.7.36.jar:/databricks/driver/kafka_2.12-3.6.2/bin/../libs/slf4j-reload4j-1.7.36.jar:/databricks/driver/kafka_2.12-3.6.2/bin/../libs/snappy-java-1.1.10.5.jar:/databricks/driver/kafka_2.12-3.6.2/bin/../libs/swagger-annotations-2.2.8.jar:/databricks/driver/kafka_2.12-3.6.2/bin/../libs/trogdor-3.6.2.jar:/databricks/driver/kafka_2.12-3.6.2/bin/../libs/zookeeper-3.8.4.jar:/databricks/driver/kafka_2.12-3.6.2/bin/../libs/zookeeper-jute-3.8.4.jar:/databricks/driver/kafka_2.12-3.6.2/bin/../libs/zstd-jni-1.5.5-1.jar (org.apache.zookeeper.ZooKeeper)\n[2024-10-20 16:31:09,434] INFO Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.ZooKeeper)\n[2024-10-20 16:31:09,435] INFO Client environment:java.io.tmpdir=/tmp (org.apache.zookeeper.ZooKeeper)\n[2024-10-20 16:31:09,436] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)\n[2024-10-20 16:31:09,436] INFO Client environment:os.name=Linux (org.apache.zookeeper.ZooKeeper)\n[2024-10-20 16:31:09,436] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)\n[2024-10-20 16:31:09,437] INFO Client environment:os.version=5.15.0-1070-aws (org.apache.zookeeper.ZooKeeper)\n[2024-10-20 16:31:09,437] INFO Client environment:user.name=root (org.apache.zookeeper.ZooKeeper)\n[2024-10-20 16:31:09,437] INFO Client environment:user.home=/root (org.apache.zookeeper.ZooKeeper)\n[2024-10-20 16:31:09,437] INFO Client environment:user.dir=/databricks/driver (org.apache.zookeeper.ZooKeeper)\n[2024-10-20 16:31:09,437] INFO Client environment:os.memory.free=937MB (org.apache.zookeeper.ZooKeeper)\n[2024-10-20 16:31:09,437] INFO Client environment:os.memory.max=1024MB (org.apache.zookeeper.ZooKeeper)\n[2024-10-20 16:31:09,437] INFO Client environment:os.memory.total=1024MB (org.apache.zookeeper.ZooKeeper)\n[2024-10-20 16:31:09,463] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@1af7f54a (org.apache.zookeeper.ZooKeeper)\n[2024-10-20 16:31:09,490] INFO jute.maxbuffer value is 4194304 Bytes (org.apache.zookeeper.ClientCnxnSocket)\n[2024-10-20 16:31:09,512] INFO zookeeper.request.timeout value is 0. feature enabled=false (org.apache.zookeeper.ClientCnxn)\n[2024-10-20 16:31:09,524] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)\n[2024-10-20 16:31:09,539] INFO Opening socket connection to server localhost/127.0.0.1:2181. (org.apache.zookeeper.ClientCnxn)\n[2024-10-20 16:31:09,548] INFO Socket connection established, initiating session, client: /127.0.0.1:50122, server: localhost/127.0.0.1:2181 (org.apache.zookeeper.ClientCnxn)\n[2024-10-20 16:31:09,612] INFO Session establishment complete on server localhost/127.0.0.1:2181, session id = 0x10000d5fa780000, negotiated timeout = 18000 (org.apache.zookeeper.ClientCnxn)\n[2024-10-20 16:31:09,629] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient)\n[2024-10-20 16:31:10,674] INFO Cluster ID = iWR2R2VRRL29XNzLlCEXIw (kafka.server.KafkaServer)\n[2024-10-20 16:31:10,688] WARN No meta.properties file under dir /tmp/kafka-logs/meta.properties (kafka.server.BrokerMetadataCheckpoint)\n[2024-10-20 16:31:10,917] INFO KafkaConfig values: \n\tadvertised.listeners = null\n\talter.config.policy.class.name = null\n\talter.log.dirs.replication.quota.window.num = 11\n\talter.log.dirs.replication.quota.window.size.seconds = 1\n\tauthorizer.class.name = \n\tauto.create.topics.enable = true\n\tauto.include.jmx.reporter = true\n\tauto.leader.rebalance.enable = true\n\tbackground.threads = 10\n\tbroker.heartbeat.interval.ms = 2000\n\tbroker.id = 0\n\tbroker.id.generation.enable = true\n\tbroker.rack = null\n\tbroker.session.timeout.ms = 9000\n\tclient.quota.callback.class = null\n\tcompression.type = producer\n\tconnection.failed.authentication.delay.ms = 100\n\tconnections.max.idle.ms = 600000\n\tconnections.max.reauth.ms = 0\n\tcontrol.plane.listener.name = null\n\tcontrolled.shutdown.enable = true\n\tcontrolled.shutdown.max.retries = 3\n\tcontrolled.shutdown.retry.backoff.ms = 5000\n\tcontroller.listener.names = null\n\tcontroller.quorum.append.linger.ms = 25\n\tcontroller.quorum.election.backoff.max.ms = 1000\n\tcontroller.quorum.election.timeout.ms = 1000\n\tcontroller.quorum.fetch.timeout.ms = 2000\n\tcontroller.quorum.request.timeout.ms = 2000\n\tcontroller.quorum.retry.backoff.ms = 20\n\tcontroller.quorum.voters = []\n\tcontroller.quota.window.num = 11\n\tcontroller.quota.window.size.seconds = 1\n\tcontroller.socket.timeout.ms = 30000\n\tcreate.topic.policy.class.name = null\n\tdefault.replication.factor = 1\n\tdelegation.token.expiry.check.interval.ms = 3600000\n\tdelegation.token.expiry.time.ms = 86400000\n\tdelegation.token.master.key = null\n\tdelegation.token.max.lifetime.ms = 604800000\n\tdelegation.token.secret.key = null\n\tdelete.records.purgatory.purge.interval.requests = 1\n\tdelete.topic.enable = true\n\tearly.start.listeners = null\n\tfetch.max.bytes = 57671680\n\tfetch.purgatory.purge.interval.requests = 1000\n\tgroup.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.RangeAssignor]\n\tgroup.consumer.heartbeat.interval.ms = 5000\n\tgroup.consumer.max.heartbeat.interval.ms = 15000\n\tgroup.consumer.max.session.timeout.ms = 60000\n\tgroup.consumer.max.size = 2147483647\n\tgroup.consumer.min.heartbeat.interval.ms = 5000\n\tgroup.consumer.min.session.timeout.ms = 45000\n\tgroup.consumer.session.timeout.ms = 45000\n\tgroup.coordinator.new.enable = false\n\tgroup.coordinator.threads = 1\n\tgroup.initial.rebalance.delay.ms = 0\n\tgroup.max.session.timeout.ms = 1800000\n\tgroup.max.size = 2147483647\n\tgroup.min.session.timeout.ms = 6000\n\tinitial.broker.registration.timeout.ms = 60000\n\tinter.broker.listener.name = null\n\tinter.broker.protocol.version = 3.6-IV2\n\tkafka.metrics.polling.interval.secs = 10\n\tkafka.metrics.reporters = []\n\tleader.imbalance.check.interval.seconds = 300\n\tleader.imbalance.per.broker.percentage = 10\n\tlistener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL\n\tlisteners = PLAINTEXT://:9092\n\tlog.cleaner.backoff.ms = 15000\n\tlog.cleaner.dedupe.buffer.size = 134217728\n\tlog.cleaner.delete.retention.ms = 86400000\n\tlog.cleaner.enable = true\n\tlog.cleaner.io.buffer.load.factor = 0.9\n\tlog.cleaner.io.buffer.size = 524288\n\tlog.cleaner.io.max.bytes.per.second = 1.7976931348623157E308\n\tlog.cleaner.max.compaction.lag.ms = 9223372036854775807\n\tlog.cleaner.min.cleanable.ratio = 0.5\n\tlog.cleaner.min.compaction.lag.ms = 0\n\tlog.cleaner.threads = 1\n\tlog.cleanup.policy = [delete]\n\tlog.dir = /tmp/kafka-logs\n\tlog.dirs = /tmp/kafka-logs\n\tlog.flush.interval.messages = 9223372036854775807\n\tlog.flush.interval.ms = null\n\tlog.flush.offset.checkpoint.interval.ms = 60000\n\tlog.flush.scheduler.interval.ms = 9223372036854775807\n\tlog.flush.start.offset.checkpoint.interval.ms = 60000\n\tlog.index.interval.bytes = 4096\n\tlog.index.size.max.bytes = 10485760\n\tlog.local.retention.bytes = -2\n\tlog.local.retention.ms = -2\n\tlog.message.downconversion.enable = true\n\tlog.message.format.version = 3.0-IV1\n\tlog.message.timestamp.after.max.ms = 9223372036854775807\n\tlog.message.timestamp.before.max.ms = 9223372036854775807\n\tlog.message.timestamp.difference.max.ms = 9223372036854775807\n\tlog.message.timestamp.type = CreateTime\n\tlog.preallocate = false\n\tlog.retention.bytes = -1\n\tlog.retention.check.interval.ms = 300000\n\tlog.retention.hours = 168\n\tlog.retention.minutes = null\n\tlog.retention.ms = null\n\tlog.roll.hours = 168\n\tlog.roll.jitter.hours = 0\n\tlog.roll.jitter.ms = null\n\tlog.roll.ms = null\n\tlog.segment.bytes = 1073741824\n\tlog.segment.delete.delay.ms = 60000\n\tmax.connection.creation.rate = 2147483647\n\tmax.connections = 2147483647\n\tmax.connections.per.ip = 2147483647\n\tmax.connections.per.ip.overrides = \n\tmax.incremental.fetch.session.cache.slots = 1000\n\tmessage.max.bytes = 1048588\n\tmetadata.log.dir = null\n\tmetadata.log.max.record.bytes.between.snapshots = 20971520\n\tmetadata.log.max.snapshot.interval.ms = 3600000\n\tmetadata.log.segment.bytes = 1073741824\n\tmetadata.log.segment.min.bytes = 8388608\n\tmetadata.log.segment.ms = 604800000\n\tmetadata.max.idle.interval.ms = 500\n\tmetadata.max.retention.bytes = 104857600\n\tmetadata.max.retention.ms = 604800000\n\tmetric.reporters = []\n\tmetrics.num.samples = 2\n\tmetrics.recording.level = INFO\n\tmetrics.sample.window.ms = 30000\n\tmin.insync.replicas = 1\n\tnode.id = 0\n\tnum.io.threads = 8\n\tnum.network.threads = 3\n\tnum.partitions = 1\n\tnum.recovery.threads.per.data.dir = 1\n\tnum.replica.alter.log.dirs.threads = null\n\tnum.replica.fetchers = 1\n\toffset.metadata.max.bytes = 4096\n\toffsets.commit.required.acks = -1\n\toffsets.commit.timeout.ms = 5000\n\toffsets.load.buffer.size = 5242880\n\toffsets.retention.check.interval.ms = 600000\n\toffsets.retention.minutes = 10080\n\toffsets.topic.compression.codec = 0\n\toffsets.topic.num.partitions = 50\n\toffsets.topic.replication.factor = 1\n\toffsets.topic.segment.bytes = 104857600\n\tpassword.encoder.cipher.algorithm = AES/CBC/PKCS5Padding\n\tpassword.encoder.iterations = 4096\n\tpassword.encoder.key.length = 128\n\tpassword.encoder.keyfactory.algorithm = null\n\tpassword.encoder.old.secret = null\n\tpassword.encoder.secret = null\n\tprincipal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder\n\tprocess.roles = []\n\tproducer.id.expiration.check.interval.ms = 600000\n\tproducer.id.expiration.ms = 86400000\n\tproducer.purgatory.purge.interval.requests = 1000\n\tqueued.max.request.bytes = -1\n\tqueued.max.requests = 500\n\tquota.window.num = 11\n\tquota.window.size.seconds = 1\n\tremote.log.index.file.cache.total.size.bytes = 1073741824\n\tremote.log.manager.task.interval.ms = 30000\n\tremote.log.manager.task.retry.backoff.max.ms = 30000\n\tremote.log.manager.task.retry.backoff.ms = 500\n\tremote.log.manager.task.retry.jitter = 0.2\n\tremote.log.manager.thread.pool.size = 10\n\tremote.log.metadata.custom.metadata.max.bytes = 128\n\tremote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager\n\tremote.log.metadata.manager.class.path = null\n\tremote.log.metadata.manager.impl.prefix = rlmm.config.\n\tremote.log.metadata.manager.listener.name = null\n\tremote.log.reader.max.pending.tasks = 100\n\tremote.log.reader.threads = 10\n\tremote.log.storage.manager.class.name = null\n\tremote.log.storage.manager.class.path = null\n\tremote.log.storage.manager.impl.prefix = rsm.config.\n\tremote.log.storage.system.enable = false\n\treplica.fetch.backoff.ms = 1000\n\treplica.fetch.max.bytes = 1048576\n\treplica.fetch.min.bytes = 1\n\treplica.fetch.response.max.bytes = 10485760\n\treplica.fetch.wait.max.ms = 500\n\treplica.high.watermark.checkpoint.interval.ms = 5000\n\treplica.lag.time.max.ms = 30000\n\treplica.selector.class = null\n\treplica.socket.receive.buffer.bytes = 65536\n\treplica.socket.timeout.ms = 30000\n\treplication.quota.window.num = 11\n\treplication.quota.window.size.seconds = 1\n\trequest.timeout.ms = 30000\n\treserved.broker.max.id = 1000\n\tsasl.client.callback.handler.class = null\n\tsasl.enabled.mechanisms = [GSSAPI]\n\tsasl.jaas.config = null\n\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n\tsasl.kerberos.min.time.before.relogin = 60000\n\tsasl.kerberos.principal.to.local.rules = [DEFAULT]\n\tsasl.kerberos.service.name = null\n\tsasl.kerberos.ticket.renew.jitter = 0.05\n\tsasl.kerberos.ticket.renew.window.factor = 0.8\n\tsasl.login.callback.handler.class = null\n\tsasl.login.class = null\n\tsasl.login.connect.timeout.ms = null\n\tsasl.login.read.timeout.ms = null\n\tsasl.login.refresh.buffer.seconds = 300\n\tsasl.login.refresh.min.period.seconds = 60\n\tsasl.login.refresh.window.factor = 0.8\n\tsasl.login.refresh.window.jitter = 0.05\n\tsasl.login.retry.backoff.max.ms = 10000\n\tsasl.login.retry.backoff.ms = 100\n\tsasl.mechanism.controller.protocol = GSSAPI\n\tsasl.mechanism.inter.broker.protocol = GSSAPI\n\tsasl.oauthbearer.clock.skew.seconds = 30\n\tsasl.oauthbearer.expected.audience = null\n\tsasl.oauthbearer.expected.issuer = null\n\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\n\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\n\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\n\tsasl.oauthbearer.jwks.endpoint.url = null\n\tsasl.oauthbearer.scope.claim.name = scope\n\tsasl.oauthbearer.sub.claim.name = sub\n\tsasl.oauthbearer.token.endpoint.url = null\n\tsasl.server.callback.handler.class = null\n\tsasl.server.max.receive.size = 524288\n\tsecurity.inter.broker.protocol = PLAINTEXT\n\tsecurity.providers = null\n\tserver.max.startup.time.ms = 9223372036854775807\n\tsocket.connection.setup.timeout.max.ms = 30000\n\tsocket.connection.setup.timeout.ms = 10000\n\tsocket.listen.backlog.size = 50\n\tsocket.receive.buffer.bytes = 102400\n\tsocket.request.max.bytes = 104857600\n\tsocket.send.buffer.bytes = 102400\n\tssl.cipher.suites = []\n\tssl.client.auth = none\n\tssl.enabled.protocols = [TLSv1.2]\n\tssl.endpoint.identification.algorithm = https\n\tssl.engine.factory.class = null\n\tssl.key.password = null\n\tssl.keymanager.algorithm = SunX509\n\tssl.keystore.certificate.chain = null\n\tssl.keystore.key = null\n\tssl.keystore.location = null\n\tssl.keystore.password = null\n\tssl.keystore.type = JKS\n\tssl.principal.mapping.rules = DEFAULT\n\tssl.protocol = TLSv1.2\n\tssl.provider = null\n\tssl.secure.random.implementation = null\n\tssl.trustmanager.algorithm = PKIX\n\tssl.truststore.certificates = null\n\tssl.truststore.location = null\n\tssl.truststore.password = null\n\tssl.truststore.type = JKS\n\ttransaction.abort.timed.out.transaction.cleanup.interval.ms = 10000\n\ttransaction.max.timeout.ms = 900000\n\ttransaction.partition.verification.enable = true\n\ttransaction.remove.expired.transaction.cleanup.interval.ms = 3600000\n\ttransaction.state.log.load.buffer.size = 5242880\n\ttransaction.state.log.min.isr = 1\n\ttransaction.state.log.num.partitions = 50\n\ttransaction.state.log.replication.factor = 1\n\ttransaction.state.log.segment.bytes = 104857600\n\ttransactional.id.expiration.ms = 604800000\n\tunclean.leader.election.enable = false\n\tunstable.api.versions.enable = false\n\tzookeeper.clientCnxnSocket = null\n\tzookeeper.connect = localhost:2181\n\tzookeeper.connection.timeout.ms = 18000\n\tzookeeper.max.in.flight.requests = 10\n\tzookeeper.metadata.migration.enable = false\n\tzookeeper.metadata.migration.min.batch.size = 200\n\tzookeeper.session.timeout.ms = 18000\n\tzookeeper.set.acl = false\n\tzookeeper.ssl.cipher.suites = null\n\tzookeeper.ssl.client.enable = false\n\tzookeeper.ssl.crl.enable = false\n\tzookeeper.ssl.enabled.protocols = null\n\tzookeeper.ssl.endpoint.identification.algorithm = HTTPS\n\tzookeeper.ssl.keystore.location = null\n\tzookeeper.ssl.keystore.password = null\n\tzookeeper.ssl.keystore.type = null\n\tzookeeper.ssl.ocsp.enable = false\n\tzookeeper.ssl.protocol = TLSv1.2\n\tzookeeper.ssl.truststore.location = null\n\tzookeeper.ssl.truststore.password = null\n\tzookeeper.ssl.truststore.type = null\n (kafka.server.KafkaConfig)\n[2024-10-20 16:31:11,357] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n[2024-10-20 16:31:11,370] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n[2024-10-20 16:31:11,383] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n[2024-10-20 16:31:11,399] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n[2024-10-20 16:31:11,469] INFO Log directory /tmp/kafka-logs not found, creating it. (kafka.log.LogManager)\n[2024-10-20 16:31:11,549] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)\n[2024-10-20 16:31:11,568] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)\n[2024-10-20 16:31:11,611] INFO Loaded 0 logs in 59ms (kafka.log.LogManager)\n[2024-10-20 16:31:11,630] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)\n[2024-10-20 16:31:11,635] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)\n[2024-10-20 16:31:11,843] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)\n[2024-10-20 16:31:11,945] INFO [feature-zk-node-event-process-thread]: Starting (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)\n[2024-10-20 16:31:11,974] INFO Feature ZK node at path: /feature does not exist (kafka.server.FinalizedFeatureChangeListener)\n[2024-10-20 16:31:12,082] INFO [zk-broker-0-to-controller-forwarding-channel-manager]: Starting (kafka.server.BrokerToControllerRequestThread)\n[2024-10-20 16:31:13,571] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)\n[2024-10-20 16:31:13,661] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)\n[2024-10-20 16:31:13,685] INFO [zk-broker-0-to-controller-alter-partition-channel-manager]: Starting (kafka.server.BrokerToControllerRequestThread)\n[2024-10-20 16:31:13,798] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2024-10-20 16:31:13,807] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2024-10-20 16:31:13,818] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2024-10-20 16:31:13,822] INFO [ExpirationReaper-0-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2024-10-20 16:31:13,835] INFO [ExpirationReaper-0-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2024-10-20 16:31:13,918] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)\n[2024-10-20 16:31:13,942] INFO [AddPartitionsToTxnSenderThread-0]: Starting (kafka.server.AddPartitionsToTxnManager)\n[2024-10-20 16:31:13,987] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient)\n[2024-10-20 16:31:14,090] INFO Stat of the created znode at /brokers/ids/0 is: 25,25,1729441874053,1729441874053,1,0,0,72058513068130304,254,0,25\n (kafka.zk.KafkaZkClient)\n[2024-10-20 16:31:14,096] INFO Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://1020-152736-6tbvonfb-10-172-219-243:9092, czxid (broker epoch): 25 (kafka.zk.KafkaZkClient)\n[2024-10-20 16:31:14,408] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2024-10-20 16:31:14,456] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient)\n[2024-10-20 16:31:14,477] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2024-10-20 16:31:14,490] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2024-10-20 16:31:14,528] INFO Feature ZK node created at path: /feature (kafka.server.FinalizedFeatureChangeListener)\n[2024-10-20 16:31:14,718] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator)\n[2024-10-20 16:31:14,735] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator)\n[2024-10-20 16:31:14,799] INFO [MetadataCache brokerId=0] Updated cache from existing None to latest Features(version=3.6-IV2, finalizedFeatures={}, finalizedFeaturesEpoch=0). (kafka.server.metadata.ZkMetadataCache)\n[2024-10-20 16:31:14,855] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)\n[2024-10-20 16:31:14,863] INFO [TxnMarkerSenderThread-0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)\n[2024-10-20 16:31:14,863] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)\n[2024-10-20 16:31:15,217] INFO [ExpirationReaper-0-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2024-10-20 16:31:15,244] INFO [Controller id=0, targetBrokerId=0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)\n[2024-10-20 16:31:15,277] WARN [Controller id=0, targetBrokerId=0] Connection to node 0 (1020-152736-6tbvonfb-10-172-219-243/127.0.1.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)\n[2024-10-20 16:31:15,337] INFO [Controller id=0, targetBrokerId=0] Client requested connection close from node 0 (org.apache.kafka.clients.NetworkClient)\n[2024-10-20 16:31:15,441] INFO [Controller id=0, targetBrokerId=0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)\n[2024-10-20 16:31:15,441] WARN [Controller id=0, targetBrokerId=0] Connection to node 0 (1020-152736-6tbvonfb-10-172-219-243/127.0.1.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)\n[2024-10-20 16:31:15,452] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)\n[2024-10-20 16:31:15,452] INFO [Controller id=0, targetBrokerId=0] Client requested connection close from node 0 (org.apache.kafka.clients.NetworkClient)\n[2024-10-20 16:31:15,477] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Enabling request processing. (kafka.network.SocketServer)\n[2024-10-20 16:31:15,485] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)\n[2024-10-20 16:31:15,545] INFO Kafka version: 7.4.0-ccs (org.apache.kafka.common.utils.AppInfoParser)\n[2024-10-20 16:31:15,545] INFO Kafka commitId: 30969fa33c185e88 (org.apache.kafka.common.utils.AppInfoParser)\n[2024-10-20 16:31:15,545] INFO Kafka startTimeMs: 1729441875537 (org.apache.kafka.common.utils.AppInfoParser)\n[2024-10-20 16:31:15,554] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)\n[2024-10-20 16:31:16,004] INFO [zk-broker-0-to-controller-alter-partition-channel-manager]: Recorded new controller, from now on will use node 1020-152736-6tbvonfb-10-172-219-243:9092 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread)\n[2024-10-20 16:31:16,079] INFO [zk-broker-0-to-controller-forwarding-channel-manager]: Recorded new controller, from now on will use node 1020-152736-6tbvonfb-10-172-219-243:9092 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread)\n[2024-10-20 16:31:52,261] INFO Creating topic meu-topico with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient)\n[2024-10-20 16:31:52,509] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(meu-topico-0) (kafka.server.ReplicaFetcherManager)\n[2024-10-20 16:31:52,698] INFO [LogLoader partition=meu-topico-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2024-10-20 16:31:52,754] INFO Created log for partition meu-topico-0 in /tmp/kafka-logs/meu-topico-0 with properties {} (kafka.log.LogManager)\n[2024-10-20 16:31:52,758] INFO [Partition meu-topico-0 broker=0] No checkpointed highwatermark is found for partition meu-topico-0 (kafka.cluster.Partition)\n[2024-10-20 16:31:52,763] INFO [Partition meu-topico-0 broker=0] Log loaded for partition meu-topico-0 with initial high watermark 0 (kafka.cluster.Partition)\n"
     ]
    }
   ],
   "source": [
    "%sh\n",
    "./kafka_2.12-3.6.2/bin/kafka-server-start.sh ./kafka_2.12-3.6.2/config/server.properties"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1937876184825390,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Servidor",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
