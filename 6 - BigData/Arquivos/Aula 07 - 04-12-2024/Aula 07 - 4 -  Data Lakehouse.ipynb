{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2267eb99",
   "metadata": {},
   "source": [
    "# Data Lakehouse\n",
    " \n",
    "## Um pouco de história\n",
    "Primeiramente surgiram os *Data Warehouses*: bases de dados para sistemas OLAPs que desenhados com modelagem *Star Schema* ou *Snowflake*, visavam organizar dados por assunto e propiciar melhores consultas se comparados a transacionais (OLTPs). Os *DWs* costumavam ser construídos em bancos de dados relacionais, como Oracle e Teradata, e por isso se limitavam a dados estruturados.\n",
    " \n",
    "Com o crescente aumento do volume de dados gerados e manipulados pelas empresas, além do valor a ser extraído de dados semi e não estruturados, bancos de dados - e consequentemente dos *DWs* portanto - passaram a não ser mais suficientes. Assim nasceram os *Data Lakes*: repositórios que, organizados em camadas, são capazes de armazenar todo e qualquer tipo de dado. \n",
    "\n",
    "Todavia, a necessidade de organizar dados de diversas fontes por assunto e conectar a ferramentas de visualização ainda se fazia necessária, tornando os *DWs* ainda relevantes. Chegamos então a uma arquitetura como essa: dados de diversas fontes são coletados e armazenados na primeira camada de um *Data Lake* através de uma ferramenta de *ingestão*. Conforme os dados avançaram pelas camadas são tratados e transformados até, por fim, serem inseridos em uma ferramenta de *DW* distribuído (que resolve as limitações de um SGBD tradicional com relação a volumetria), como Redshift da AWS ou Synapse Analytics (antigo Azure DW) da Microsoft, para então alimentarem dashboards.\n",
    " \n",
    "Arquiteturas similares à descrita são funcionais e encontradas em diversos ambientes modernos. Entretanto, ela apresenta alguns problemas:\n",
    "- Com a necessidade de mover dados de um *Data Lake* para um *Data Warehouse*, custos são impostos sobre o uso de ferramentas de ingestão para realizar tais cargas.\n",
    "- Estando os mesmos dados presentes no *Data Lake* e em um *Data Warehouse*, há duplicidade que implica em custos extras de armazenamento.\n",
    " \n",
    "Existe alguma arquitetura que resolva esses pontos?\n",
    " \n",
    "Tomando como base os assuntos discutidos no curso até aqui, a primeira resposta que talvez lhe venha à mente seja: Apache Hive, ferramenta do ecossistema Hadoop que permite a execução de queries sobre arquivos, além da gestão de metadados para tratá-los como tabelas e bancos de dados. E de fato o Hive é uma solução muito interessante, que não se limita ao ecossistema Hadoop (seja via Databricks ou não) e poderia permitir a criação de um DW sobre uma camada do *Data Lake*, para isso foi criado. Mas o Hive possui aspectos negativos, como não promover transações *ACID*.\n",
    " \n",
    "### ACID\n",
    "**ACID** são propriedades de transação de um banco de dados que visam garantir confiabilidade, visto que com múltiplos usuários simultâneos é muito comum encontrarmos concorrência de acesso a objetos e tabelas.\n",
    "- Atomicidade: em uma transação composta por duas ou mais partes, ou a transação é executada por completo ou não será.\n",
    "- Consistência: se a transação não criar um estado válido dos dados ao seu final, seu estado anterior deverá ser restabelecido.\n",
    "- Isolamento: uma transação ainda em andamento deve se manter isolada de outras operações, evitando interferências por concorrência.\n",
    "- Durabilidade: dados validados são registrados de forma que mesmo em caso de falha futura ou reinício do sistema eles se manterão corretos.\n",
    "![ACID](https://s3-sa-east-1.amazonaws.com/lcpi/6054ff49-9667-45ac-88d9-a339e5973e0b.png)\n",
    "Fonte: [Databricks](https://databricks.com/glossary/acid-transactions)\n",
    " \n",
    "## Data Lakehouse\n",
    "Unindo o melhor de ambos os mundos: a flexibilidade e baixo custo de armazenamento de um *Data Lake* e organização de um *Data Warehouse*, surgem os **Data Lakehouses**. Capazes então de consultar dados fisicamente em um *DL*, como o Apache Hive, mas trazendo mais confiabilidade através de transações **ACID**, uma arquitetura como esta pode ser implementada utilizando Redshift Spectrum da AWS, Snowflake ou Delta Tables do Databricks, por exemplo.\n",
    " \n",
    "![Data Lakehouse](https://s3-sa-east-1.amazonaws.com/lcpi/7db0a57b-60ab-4f56-a350-d2e145de88d1.png)\n",
    "Fonte: [Databricks](https://databricks.com/glossary/data-lakehouse)\n",
    " \n",
    "### Delta tables\n",
    "Na plataforma Databricks é possível implementar um **Data Lakehouse** através de Delta Tables. Para criá-las, basta utilizar a cláusula `USING` já vista, seguida de `DELTA`:\n",
    "~~~\n",
    "CREATE TABLE <nome_tabela> (<campos e tipos de dados>)\n",
    "USING DELTA;\n",
    "~~~\n",
    " \n",
    "A tabela criada estará disponível no menu Data a direita, bem como o banco de dados onde foi inserida, e terá seus arquivos armazenados em *parquet* com compressão *snappy*. \n",
    " \n",
    "#### Vantagens\n",
    "Além de todas as vantagens já descritas de um **Data Lakehouse**, há mais um aspecto interessante das **Delta Tables** no *Databricks*: enquanto tabelas particionadas que usam hive precisam ter seus metadados atualizados todas as vezes que recebem novos dados (*MSCK REPAIR TABLE*), tabelas criadas usando **Delta** não precisam!\n",
    " \n",
    "## Indicações e Bibliografia\n",
    "[Mentalidade Data-driven](https://letscode.com.br/blog/mentalidade-data-driven-entenda-a-importancia-para-sua-empresa)\n",
    " \n",
    "[Transações ACID](https://databricks.com/glossary/acid-transactions)\n",
    " \n",
    "[Amazon Redshift Spectrum](https://docs.aws.amazon.com/pt_br/redshift/latest/dg/c-using-spectrum.html)\n",
    " \n",
    "[Data Lakehouse - Snowflake](https://www.snowflake.com/guides/what-data-lakehouse)\n",
    " \n",
    "[Data Lakehouse - Databricks](https://databricks.com/glossary/data-lakehouse)\n",
    " \n",
    "[Data Lakehouse - GCP](https://cloud.google.com/blog/products/data-analytics/open-data-lakehouse-on-google-cloud)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
